{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "\n",
    "## 1. Decision Tree Implementation\n",
    "\n",
    "### 1.1. Learning algorithm\n",
    "We want to learn the decision tree from data.\n",
    "\n",
    "To implement the ID3 algorithm - we need a tree data structure.\n",
    "The first thing to do is to define a `Node` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(self, col=-1, value=None, results=None, tb=None, fb=None):\n",
    "        self.col = col # attribute on which to split\n",
    "        self.value = value # value on which to split\n",
    "        self.results = results # If the node has no children - we store here class labels with their counts\n",
    "        self.tb = tb  # True branch\n",
    "        self.fb = fb  # False branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing set of observations `rows` based on the value `value` in column `column`.\n",
    "It can split on numeric value (greater than or equal) or on categorical value (equal/not equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(rows, column, value):\n",
    "    # define split function according to the value type\n",
    "    split_function = None\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        split_function = lambda row: row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row: row[column] == value\n",
    "\n",
    "    # Divide the rows into two sets and return them\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "    return (set1, set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the quality of the split, we need to count the occurrence of each class label in the current subset (potential tree node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(rows):\n",
    "    label_count = {}\n",
    "    for row in rows:\n",
    "        # The class label is in the last column\n",
    "        label = row[- 1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different methods to evaluate the homogeneity of the target variable within a given subset generated by split.\n",
    "\n",
    "* GINI:  probability that two items randomly selected fall into the same class. We use here GINI impurity, because we want to use the same code which chooses the best split based on the **minimum** of the score. GINI impurity = 1.0 - GINI.\n",
    "* Entropy: the sum of $-p(x)\\log(p(x))$ across all the different possible classes $x$.\n",
    "* Variance: variance is used if the target variable can be considered a number. In this case we build a special type of decision tree - a **Regression Tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def gini_impurity(rows):\n",
    "    total = len(rows)\n",
    "    counts = count_labels(rows)\n",
    "    gini = 0\n",
    "    for key, val in counts.items():\n",
    "        p = val / total\n",
    "        gini += p*p\n",
    "        \n",
    "    return (1 - gini)\n",
    "\n",
    "def entropy(rows):\n",
    "    total = len(rows)\n",
    "    counts = count_labels(rows)\n",
    "    ent = 0.0\n",
    "    for key,val in counts.items():\n",
    "        p = val / total\n",
    "        ent = ent - p * log(p, 2)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def variance(rows):\n",
    "    if len(rows) == 0: return 0\n",
    "    num_label = [float(row[- 1]) for row in rows]\n",
    "    mean = sum(num_label) / len(num_label)\n",
    "    variance = sum([(d - mean) ** 2 for d in num_label]) / len(num_label)\n",
    "    return variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to build the decision tree from data.\n",
    "\n",
    "The `buildtree` function below takes as an input all the rows of the dataset and returns a decision tree with the root  implemented as `DecisionNode` class.\n",
    "\n",
    "The default `score_func` to test the quality of the split is entropy, but any other score can be applied by passing any of the scoring functions above. Remember that `variance` can only be applied if the target variable is numeric (Regression Tree).\n",
    "\n",
    "To avoid overfitting, the algorithm uses additional stop criteria: if the improvement of the score is less than `min_improvement` or if the number of samples in every node is less than `min_samples`, then the splitting of samples stops. For the sake of the visual demo, there is also a parameter called `max_depth`, which controls the maximum depth of the tree. \n",
    "\n",
    "With the default values the algorithm will continue until the score of the split stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildtree(rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=None, depth=0):\n",
    "    if len(rows) == 0:\n",
    "        return DecisionNode()\n",
    "    # Compute overall score for the entire rows dataset\n",
    "    current_score = score_func(rows)\n",
    "\n",
    "    # Set up accumulator variables to track the best split criteria\n",
    "    best_score = current_score\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    \n",
    "    # Total number of features - except the last column where we store the class (target)\n",
    "    column_count = len(rows[0]) - 1 \n",
    "    for col in range(0, column_count):\n",
    "        # Generate the list of unique values in\n",
    "        # this column to split on them\n",
    "        column_values = set()\n",
    "        for row in rows:\n",
    "            column_values.add(row[col])\n",
    "            \n",
    "        # Now try splitting the rows \n",
    "        # on each unique value in this column\n",
    "        for value in column_values:\n",
    "            (set1, set2) = split(rows, col, value)\n",
    "\n",
    "            # Evaluate the quality of the split\n",
    "            # p is the proportion of subset set1 \n",
    "            p = float(len(set1)) / len(rows)\n",
    "            split_score = p * score_func(set1) + (1-p) * score_func(set2)\n",
    "            \n",
    "            if split_score < best_score and \\\n",
    "                (len(set1) > min_samples and len(set2) > min_samples) and \\\n",
    "                (current_score - split_score) > min_improvement:\n",
    "                best_score = split_score\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "\n",
    "    # Create the sub branches\n",
    "    if (current_score - best_score) > min_improvement and \\\n",
    "        (max_depth is None or depth < max_depth) :\n",
    "        # print(\"Splitting on\",best_criteria, \" 2 sets:\", len(best_sets[0]),len(best_sets[1]))\n",
    "        true_branch = buildtree(best_sets[0], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        false_branch = buildtree(best_sets[1], score_func, min_improvement, min_samples, max_depth, depth+1)\n",
    "        return DecisionNode(col=best_criteria[0], value=best_criteria[1],\n",
    "                            tb=true_branch, fb=false_branch)\n",
    "    else: # Done splitting - summarize class labels in leaf nodes\n",
    "        return DecisionNode(results=count_labels(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using learned tree for classification\n",
    "\n",
    "### 2.1. Classifying fruits\n",
    "\n",
    "First, build the tree from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fruits with their size and color\n",
    "fruits = [\n",
    "    [4, 'red', 'apple'],\n",
    "    [4, 'green', 'apple'],\n",
    "    [1.5, 'red', 'cherry'],\n",
    "    [1, 'green', 'grape'],\n",
    "    [5, 'red', 'apple'],\n",
    "    [1.2, 'red', 'grape']\n",
    "]\n",
    "\n",
    "tree = buildtree(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prediction` function below outputs the label of the leaf node according to a prevalent class.\n",
    "It cannot be used for regression trees - you need to implement your own function if you want to use this code for predicting a numeric target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(leaf_labels):\n",
    "    total = 0\n",
    "    result = {}\n",
    "    for label, count in leaf_labels.items():\n",
    "        total += count\n",
    "        result[label] = count\n",
    "\n",
    "    for label, val in result.items():\n",
    "        result[label] = str(int(result[label]/total * 100))+\"%\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to display the tree: **print with indent** and **tree drawing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, current_branch, attributes=None,  indent='', leaf_funct=prediction):\n",
    "    # Is this a leaf node?\n",
    "    if tree.results != None:\n",
    "        print(indent + current_branch + str(leaf_funct(tree.results)))\n",
    "    else:\n",
    "        # Print the split question\n",
    "        split_col = str(tree.col)\n",
    "        if attributes is not None:\n",
    "            split_col = attributes[tree.col]\n",
    "        split_val = str(tree.value)\n",
    "        if type(tree.value) == int or type(tree.value) == float:\n",
    "            split_val = \">=\" + str(tree.value)\n",
    "        print(indent + current_branch + split_col + ': ' + split_val + '? ')\n",
    "\n",
    "        # Print the branches\n",
    "        indent = indent + '  '\n",
    "        print_tree(tree.tb, 'T->', attributes, indent)\n",
    "        print_tree(tree.fb, 'F->', attributes, indent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', [\"size\", \"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def getwidth(tree):\n",
    "    if tree.tb == None and tree.fb == None: return 1\n",
    "    return getwidth(tree.tb) + getwidth(tree.fb)\n",
    "\n",
    "\n",
    "def getdepth(tree):\n",
    "    if tree.tb == None and tree.fb == None: return 0\n",
    "    return max(getdepth(tree.tb), getdepth(tree.fb)) + 1\n",
    "\n",
    "\n",
    "def drawtree(tree, jpeg='tree.jpg', attributes=None):\n",
    "    w = getwidth(tree) * 100\n",
    "    h = getdepth(tree) * 100 + 120\n",
    "\n",
    "    img = Image.new('RGB', (w, h), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    drawnode(draw, tree, w / 2, 20, attributes)\n",
    "    img.save(jpeg, 'JPEG')\n",
    "\n",
    "\n",
    "def drawnode(draw, tree, x, y, attributes=None):\n",
    "    if tree.results == None:\n",
    "        # Get the width of each branch\n",
    "        w1 = getwidth(tree.fb) * 100\n",
    "        w2 = getwidth(tree.tb) * 100\n",
    "\n",
    "        # Determine the total space required by this node\n",
    "        left = x - (w1 + w2) / 2\n",
    "        right = x + (w1 + w2) / 2\n",
    "\n",
    "        # Draw the condition string\n",
    "        if attributes is not None:\n",
    "            draw.text((x - 20, y - 10), str(attributes[tree.col])[0:5] + ':' + str(tree.value), (0, 0, 0))\n",
    "        else:\n",
    "            draw.text((x - 20, y - 10), str(tree.col)[0:5] + ':' + str(tree.value), (0, 0, 0))        \n",
    "\n",
    "        # Draw links to the branches\n",
    "        draw.line((x, y, left + w1 / 2, y + 100), fill=(255, 0, 0))\n",
    "        draw.line((x, y, right - w2 / 2, y + 100), fill=(255, 0, 0))\n",
    "\n",
    "        # Draw the branch nodes\n",
    "        drawnode(draw, tree.fb, left + w1 / 2, y + 100, attributes)\n",
    "        drawnode(draw, tree.tb, right - w2 / 2, y + 100, attributes)\n",
    "    else:\n",
    "        txt = ' \\n'.join(['%s:%d' % v for v in tree.results.items()])\n",
    "        draw.text((x - 20, y), txt, (0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawtree(tree, jpeg='fruits_dt.jpg', attributes=[\"size\", \"color\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image               # to load images\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "pil_im = Image.open('fruits_dt.jpg')\n",
    "display(pil_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification is done by traversing the tree starting at the root and selecting the subtree according to the values of the attributes in a sample that we are trying to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(observation, tree):\n",
    "    if tree.results != None:\n",
    "        return prediction(tree.results)\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        branch = None\n",
    "        if isinstance(v, int) or isinstance(v, float):\n",
    "            if v >= tree.value:\n",
    "                branch = tree.tb\n",
    "            else:\n",
    "                branch = tree.fb\n",
    "        else:\n",
    "            if v == tree.value:\n",
    "                branch = tree.tb\n",
    "            else:\n",
    "                branch = tree.fb\n",
    "        return classify(observation, branch)\n",
    "\n",
    "\n",
    "# Classify an observation with missing data\n",
    "def mdclassify(observation, tree):\n",
    "    if tree.results != None:\n",
    "        return prediction(tree.results)\n",
    "    else:\n",
    "        v = observation[tree.col]\n",
    "        if v == None:\n",
    "            tr, fr = mdclassify(observation, tree.tb), mdclassify(observation, tree.fb)\n",
    "            tcount = sum(tr.values())\n",
    "            fcount = sum(fr.values())\n",
    "            tw = float(tcount) / (tcount + fcount)\n",
    "            fw = float(fcount) / (tcount + fcount)\n",
    "            result = {}\n",
    "            for k, v in tr.items(): result[k] = v * tw\n",
    "            for k, v in fr.items(): result[k] = v * fw\n",
    "            return result\n",
    "        else:\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v >= tree.value:\n",
    "                    branch = tree.tb\n",
    "                else:\n",
    "                    branch = tree.fb\n",
    "            else:\n",
    "                if v == tree.value:\n",
    "                    branch = tree.tb\n",
    "                else:\n",
    "                    branch = tree.fb\n",
    "            return mdclassify(observation, branch)\n",
    "\n",
    "def max_depth(tree):\n",
    "    if tree.results != None:\n",
    "        return 0\n",
    "    else:\n",
    "        # Compute the depth of each subtree\n",
    "        tDepth = max_depth(tree.tb)\n",
    "        fDepth = max_depth(tree.fb)\n",
    "\n",
    "        # Use the larger one\n",
    "        if (tDepth > fDepth):\n",
    "            return tDepth + 1\n",
    "        else:\n",
    "            return fDepth + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Titanic: predicting survival\n",
    "\n",
    "Let's try to learn which attributes were important for survival of Titanic passengers. We use the same [titanic](https://docs.google.com/spreadsheets/d/1W0TCr30twkLtCuBl_ryCqBtkfiCP98tAOVi5AebMh34/edit?usp=sharing) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../../data_ml_2020/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take only a subset of categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Pclass', 'Sex', 'Age', 'Survived'] ]\n",
    "print(data.columns)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the demo, remove all the records with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(how=\"any\")\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom algorithm takes as an input a regular Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = data.to_numpy().tolist()\n",
    "print(data_rows[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column is treated as a class label by our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = data.columns.to_numpy().tolist()\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildtree(data_rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coronavirus risk factors\n",
    "\n",
    "As discussed in the lecture, decision trees can be used not only for classification, but also to find out which atttributes are most important in classifying the record into a specific class. In this part we want to find out which symptoms/chronic conditions contribute most to the deadly outcome from catching COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tis Mexican dataset which contains the information from the Statistical Yearbooks of Morbidity 2015-2017 (as well as the information regarding cases associated with COVID-19) was found on [kaggle](https://www.kaggle.com/tanmoyx/covid19-patient-precondition-dataset).\n",
    "\n",
    "Download the preprocessed dataset which contains only patients that tested positive for COVID-19 and with symptom atributes converted to categorical: [link](https://docs.google.com/spreadsheets/d/1EPewR1KdT8mszXJMuqELRTHAVIFHEJw9VwLsb9whKPI/edit?usp=sharing).\n",
    "\n",
    "In this dataset we have the following attributes:\n",
    "1. sex: 1 -woman, 2-man\n",
    "2. age: numeric\n",
    "3. diabetes: yes/no\n",
    "4. copd (chronic obstructive pulmonary disease): yes/no\n",
    "5. asthma: yes/no\n",
    "6. imm_supr (suppressed immune system): yes/no\n",
    "7. hypertension: yes/no\n",
    "8. cardiovascular: yes/no\n",
    "9. renal_chronic: yes/no\n",
    "10. tobacco: yes/no\t\n",
    "11. outcome: alive/dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../../data_ml_2020/covid_categorical_good.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "data = data.dropna(how=\"any\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = data.to_numpy().tolist()\n",
    "len(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = data.columns.to_numpy().tolist()\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Using custom decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = buildtree(data_rows, score_func=entropy, min_improvement=0, min_samples=0, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree, '', columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Using sklearn\n",
    "\n",
    "The decision tree algorithm in sklearn library is not implemented very well. It requires all the attributes to be numeric - while decision trees work best with the categorical attributes. The dataset for this part contains all numeric attributes and can be found [here](https://docs.google.com/spreadsheets/d/1FHTP2RtclUg05GztDW-diMbajMAynPnECW8hyjNLUys/edit?usp=sharing).\n",
    "\n",
    "In this dataset the binary yes/no attributes are presented as:\n",
    "* YES: 1\n",
    "* NO: 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file =  \"../../data_ml_2020/covid_numeric.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['sex', 'diabetes', 'copd', 'asthma', 'imm_supr', 'hypertension',\n",
    "       'cardiovascular', 'obesity', 'renal_chronic', 'tobacco']:\n",
    "    data_num[k].replace({97: np.nan, 98: np.nan, 99: np.nan}, inplace=True)\n",
    "\n",
    "\n",
    "data_num = data_num.dropna()\n",
    "len(data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if there is a correlation between any of the attributes and the outcome. We will replace the \"alive\" outcome with number 0, and \"dead\" with number 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_num_ = data_num.copy()\n",
    "conditions = [ data_num_['outcome'].eq(\"alive\"), data_num_['outcome'].eq('dead')]\n",
    "choices = [0,2]\n",
    "\n",
    "data_num_['outcome'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "corr = data_num_.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide dataset into features and class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_num.loc[:, data_num.columns != 'outcome']\n",
    "print(X.columns)\n",
    "\n",
    "Y = data_num.loc[:, data_num.columns == 'outcome']\n",
    "print(Y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset intro training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parameters can be specified to build the model:\n",
    "\n",
    "`model = tree.DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        max_depth=None, \n",
    "        min_samples_split=2, \n",
    "        min_samples_leaf=1, \n",
    "        max_features=None, \n",
    "        random_state=None, \n",
    "        min_density=None, \n",
    "        compute_importances=None, \n",
    "        max_leaf_nodes=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 7)\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = data.columns.to_numpy().tolist()\n",
    "from sklearn.tree import export_text\n",
    "r = export_text(model, feature_names=columns_list[:-1])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important comorbidity factors? Hard to tell. \n",
    "We will try to discover them more efficiently in the next lab using classification rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 Marina Barsky. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
